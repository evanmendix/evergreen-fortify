import os
import re
import glob
import shutil
import fitz  # PyMuPDF
from collections import defaultdict
from ..utils.get_filepath import REPORTS_DIR
from datetime import datetime

# Determine the project root dynamically
# Assuming the script is in src/fortify_tool/actions/
# Project root is 4 levels up from this file.
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.abspath(os.path.join(SCRIPT_DIR, '..', '..', '..', ))


def clean_filename(text):
    """
    å°‡æ–‡å­—è½‰æ›ç‚ºåˆæ³•çš„æª”æ¡ˆåç¨±ã€‚
    """
    cleaned = re.sub(r'[\\/*?:"<>|]', '', text)
    cleaned = re.sub(r'\s+', '_', cleaned)
    cleaned = re.sub(r'_+', '_', cleaned)
    cleaned = cleaned.strip('_')
    return cleaned[:100] if len(cleaned) > 100 else cleaned


def format_content_for_markdown(text):
    """
    ä½¿ç”¨ç‹€æ…‹æ©Ÿå°‹æ‰¾ 'Source:' æˆ– 'Sink:' å¾Œçš„æ–‡å­—å€å¡Šï¼Œä¸¦ç”¨ Markdown ç¨‹å¼ç¢¼åœæ¬„åŒ…è¦†ã€‚
    """
    lines = text.split('\n')
    processed_lines = []
    in_code_block = False
    for line in lines:
        stripped_line = line.strip()
        if stripped_line.startswith('Source:') or stripped_line.startswith('Sink:'):
            if in_code_block:
                processed_lines.append('```')
            processed_lines.append(line)
            processed_lines.append('```')
            in_code_block = True
        elif stripped_line == '' and in_code_block:
            processed_lines.append('```')
            processed_lines.append(line)
            in_code_block = False
        else:
            processed_lines.append(line)
    if in_code_block:
        processed_lines.append('```')
    return '\n'.join(processed_lines)


def clean_category_content(text):
    """
    æ¸…ç†å¾ PDF æå–å‡ºçš„å–®ä¸€ Category å…§å®¹ï¼Œç§»é™¤é é¦–ã€é å°¾ã€é™„éŒ„åŠä¸å¿…è¦çš„åœ–è¡¨/çµ±è¨ˆæ•¸å­—ã€‚
    """
    # ç§»é™¤é™„éŒ„ä¹‹å¾Œçš„æ‰€æœ‰å…§å®¹
    appendix_keywords = ['ä¾æ“šé¡åˆ¥æ’åºåˆ—å‡ºå•é¡Œ', 'æª¢æ¸¬ç¸½æª”æ¡ˆæ•¸', 'æƒææª”æ¡ˆæ¸…å–®']
    for keyword in appendix_keywords:
        if keyword in text:
            text = text.split(keyword)[0]

    lines = text.split('\n')
    cleaned_lines = []

    # Regex to detect and remove lines that are likely artifacts from charts or tables.
    chart_data_pattern = re.compile(
        r'^\s*(\d+\s*)+$|'  # e.g., "10", "0 5 10"
        r'^\s*(Critical|High|Medium|Low)\s*$|'  # e.g., "Critical"
        r'.*Issues\s+Found.*'  # e.g., "Issues Found" table header
    )

    for line in lines:
        # ç§»é™¤é é¦–/é å°¾
        if 'FIA_Fortify_Summary_Report' in line or 'ç¨‹å¼ç¢¼å®‰å…¨æª¢æ¸¬' in line:
            continue
        # ç§»é™¤é ç¢¼
        if re.match(r'^\s*Page \d+ of \d+\s*$', line.strip()):
            continue

        # ç§»é™¤åœ–è¡¨/çµ±è¨ˆè¡Œ
        if chart_data_pattern.match(line.strip()):
            continue

        # å¾ä¸»è¦ Category æ¨™é¡Œä¸­ç§»é™¤å•é¡Œè¨ˆæ•¸ï¼Œä¾‹å¦‚ " (1 Issues, 1 High)"
        if line.strip().startswith('Category:'):
            line = re.sub(r'\s*\(\d+\s+Issues?.*?\)\s*$', '', line)

        cleaned_lines.append(line)

    cleaned_text = '\n'.join(cleaned_lines)

    # ç§»é™¤é€£çºŒçš„ç©ºè¡Œï¼Œä¿æŒå ±å‘Šæ•´æ½”
    cleaned_text = re.sub(r'\n{3,}', '\n\n', cleaned_text)

    return cleaned_text.strip()


def load_solutions(solutions_dir='data/solutions'):
    """
    å¾æŒ‡å®šç›®éŒ„è¼‰å…¥æ‰€æœ‰è§£æ±ºæ–¹æ¡ˆæª”æ¡ˆï¼Œå€åˆ†é€šç”¨è§£æ³•èˆ‡å°ˆå±¬è§£æ³•ã€‚
    - é€šç”¨è§£æ³•ï¼šä»¥åš´é‡æ€§ (å¦‚ 'critical') ç‚ºéµã€‚
    - å°ˆå±¬è§£æ³•ï¼šä»¥æ¨™æº–åŒ–çš„å¼±é»åç¨± (å¦‚ 'Cross-Site_Scripting_DOM') ç‚ºéµï¼Œå€¼ç‚ºåŒ…å«å…§å®¹å’Œåš´é‡æ€§çš„å­—å…¸ã€‚
    è¿”å›å…©å€‹å­—å…¸: generic_solutions, specific_solutions
    """
    generic_solutions = {}
    specific_solutions = {}
    if not os.path.isdir(solutions_dir):
        print(f"[WARN] æ‰¾ä¸åˆ°è§£æ±ºæ–¹æ¡ˆç›®éŒ„: '{solutions_dir}'")
        return generic_solutions, specific_solutions

    print(f"\n--- æ­£åœ¨å¾ '{solutions_dir}' è¼‰å…¥è§£æ±ºæ–¹æ¡ˆ ---")
    for filename in os.listdir(solutions_dir):
        if not filename.endswith('.md'):
            continue

        # 1. è™•ç†é€šç”¨è§£æ³• (ä¾åš´é‡ç­‰ç´š)
        severity_match = re.search(r'Fortify (\w+) Solution\.md', filename, re.IGNORECASE)
        if not severity_match:
            continue

        severity = severity_match.group(1).lower()
        filepath = os.path.join(solutions_dir, filename)

        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()

            # 2. è™•ç†å°ˆå±¬è§£æ³• (ä¾ ## æ¨™é¡Œ)
            # ä½¿ç”¨æ­£è¦è¡¨ç¤ºå¼åˆ†å‰²ï¼Œä¿ç•™åˆ†éš”ç¬¦ (## ...)
            parts = re.split(r'(\n## .+)', content)

            # ç¬¬ä¸€éƒ¨åˆ†æ˜¯é€šç”¨è§£æ³•å…§å®¹ (åœ¨ç¬¬ä¸€å€‹ ## ä¹‹å‰)
            if parts and parts[0].strip():
                generic_solutions[severity] = parts[0].strip()
                print(f"  [INFO] å·²è¼‰å…¥ '{severity}' ç­‰ç´šçš„é€šç”¨è§£æ±ºæ–¹æ¡ˆã€‚")

            # è™•ç†å¾ŒçºŒçš„å°ˆå±¬è§£æ³•éƒ¨åˆ†
            # parts çµæ§‹: [é€šç”¨, ## æ¨™é¡Œ1, å…§å®¹1, ## æ¨™é¡Œ2, å…§å®¹2, ...]
            specific_issues_found = 0
            for i in range(1, len(parts), 2):
                header = parts[i].strip()
                body = parts[i + 1].strip() if (i + 1) < len(parts) else ""

                # å¾ '## Cross-Site Scripting: DOM' æå– 'Cross-Site Scripting: DOM'
                issue_name = header.lstrip('# ').strip()
                normalized_name = clean_filename(issue_name)

                # å°‡æ¨™é¡Œå’Œå…§å®¹é‡æ–°çµ„åˆï¼Œä¸¦å„²å­˜åš´é‡æ€§
                specific_solutions[normalized_name] = {
                    "content": f"{header}\n\n{body}",
                    "severity": severity
                }
                specific_issues_found += 1

            if specific_issues_found > 0:
                print(f"    -> åœ¨ '{filename}' ä¸­æ‰¾åˆ°ä¸¦è¼‰å…¥ {specific_issues_found} å€‹ '{severity}' ç­‰ç´šçš„å°ˆå±¬è§£æ³•ã€‚")

        except Exception as e:
            print(f"  [WARN] è®€å–è§£æ±ºæ–¹æ¡ˆæª”æ¡ˆ '{filename}' å¤±æ•—: {e}")

    return generic_solutions, specific_solutions


def get_severity_from_header(header_line):
    """
    å¾ Category æ¨™é¡Œè¡Œä¸­æå–æœ€é«˜åš´é‡ç­‰ç´šã€‚
    ä¾‹å¦‚ï¼š'Category: ... (1 Issues, 1 High)' -> 'high'
    """
    severities = ['critical', 'high', 'medium', 'low']
    header_lower = header_line.lower()
    for severity in severities:
        if f" {severity}" in header_lower:
            return severity
    return None


def append_solution_to_file(file_path, solution_content):
    """
    å°‡è§£æ±ºæ–¹æ¡ˆå…§å®¹é™„åŠ åˆ°æŒ‡å®šçš„ Markdown æª”æ¡ˆæœ«å°¾ã€‚
    """
    try:
        with open(file_path, 'a', encoding='utf-8') as f:
            f.write('\n\n---\n')
            f.write('# é€šç”¨è§£æ±ºæ–¹æ¡ˆå»ºè­°\n\n')
            f.write(solution_content)
        return True
    except IOError as e:
        print(f"    [ERROR] é™„åŠ è§£æ±ºæ–¹æ¡ˆåˆ° '{os.path.basename(file_path)}' å¤±æ•—: {e}")
        return False


def process_pdf_file(pdf_path, generic_solutions, specific_solutions):
    """è™•ç†å–®ä¸€ PDFï¼Œæ‹†åˆ†å ±å‘Šä¸¦é™„åŠ è§£æ±ºæ–¹æ¡ˆã€‚"""
    pdf_filename = os.path.basename(pdf_path)
    project_name, _ = os.path.splitext(pdf_filename)
    print(f"--- é–‹å§‹è™•ç†å°ˆæ¡ˆ: {project_name} (ä¾†æº: {pdf_filename}) ---")

    output_dir = os.path.join(PROJECT_ROOT, 'ç”¢å‡ºè³‡æ–™', 'Fortifyå ±å‘Šæ•´ç†', 'repoæ‹†åˆ†å ±å‘Š', project_name)

    if os.path.isdir(output_dir):
        shutil.rmtree(output_dir)
    os.makedirs(output_dir)

    try:
        print(f"  [INFO] æ­£åœ¨è®€å– PDF: {pdf_filename}")
        doc = fitz.open(pdf_path)
        full_text = "".join([page.get_text("text") for page in doc])
        doc.close()
        print(f"  [INFO] PDF è®€å–å®Œæˆã€‚")

        category_pattern = r'(Category: .+?(\d+ Issues?).*?)(?=Category: |\Z)'
        matches = list(re.finditer(category_pattern, full_text, re.DOTALL))

        if not matches:
            print("  [WARN] åœ¨ PDF ä¸­æ‰¾ä¸åˆ°ä»»ä½• 'Category:' æ¨™ç±¤ï¼Œå°ˆæ¡ˆå¯èƒ½å·²å®Œå…¨ä¿®å¾©ã€‚")
            # å»ºç«‹ç‹€æ…‹èªªæ˜æª”æ¡ˆ
            status_file_path = os.path.join(output_dir, "00_å°ˆæ¡ˆç‹€æ…‹.md")
            status_content = f"""# {project_name} - å°ˆæ¡ˆç‹€æ…‹

## ğŸ‰ æ­å–œï¼æ­¤å°ˆæ¡ˆå·²å®Œå…¨ä¿®å¾©

æ ¹æ“šæœ€æ–°çš„ Fortify æƒæå ±å‘Šï¼Œæ­¤å°ˆæ¡ˆç›®å‰**æ²’æœ‰ä»»ä½•å®‰å…¨æ€§å•é¡Œ**ã€‚

- **ç‹€æ…‹**: âœ… å·²å®Œå…¨ä¿®å¾©
- **æœ€å¾Œæ›´æ–°**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **èªªæ˜**: åœ¨æœ€æ–°çš„ Fortify å ±å‘Šä¸­æœªç™¼ç¾ä»»ä½• Category æ¨™ç±¤ï¼Œè¡¨ç¤ºæ‰€æœ‰å…ˆå‰çš„å®‰å…¨æ€§å•é¡Œéƒ½å·²å¾—åˆ°è§£æ±ºã€‚

---

> [!SUCCESS]
> æ­¤å°ˆæ¡ˆå·²é”åˆ°å®‰å…¨æ€§åˆè¦æ¨™æº–ï¼Œç„¡éœ€é€²ä¸€æ­¥çš„ä¿®å¾©å·¥ä½œã€‚
"""
            with open(status_file_path, 'w', encoding='utf-8') as f:
                f.write(status_content)
            
            print(f"  [SUCCESS] å·²å»ºç«‹å°ˆæ¡ˆç‹€æ…‹èªªæ˜æª”æ¡ˆ: {os.path.basename(status_file_path)}")
            return {"project_name": project_name, "status": "fully_remediated", "issues_count": 0}

        print(f"  [INFO] æ‰¾åˆ° {len(matches)} å€‹å•é¡Œé¡åˆ¥ï¼Œé–‹å§‹å¯«å…¥æª”æ¡ˆ...")
        files_processed_count = 0
        issue_summary_data = []
        for i, match in enumerate(matches, 1):
            category_content = match.group(1).strip()
            header_line = category_content.split('\n', 1)[0]
            title_match = re.search(r'Category: (.+?)\s*\(', header_line)
            category_name = title_match.group(1).strip() if title_match else f"Unknown_Category_{i}"

            filename = clean_filename(f"{i:03d}_{category_name}.md")
            output_path = os.path.join(output_dir, filename)

            cleaned_content = clean_category_content(category_content)
            formatted_content = format_content_for_markdown(cleaned_content)

            # Count occurrences of Source and Sink to give a summary
            source_count = formatted_content.count('Source:')
            sink_count = formatted_content.count('Sink:')
            total_locations = source_count + sink_count

            # Create a summary line to show how many code locations need review
            if total_locations > 0:
                summary_line = f"> [!NOTE]\n> This report identifies **{total_locations}** code location(s) for review ({source_count} Source(s), {sink_count} Sink(s)).\n\n"
                final_content = summary_line + formatted_content
            else:
                final_content = formatted_content

            with open(output_path, 'w', encoding='utf-8') as out_f:
                out_f.write(final_content)

            # é™„åŠ è§£æ±ºæ–¹æ¡ˆ (æ–°é‚è¼¯ï¼šå„ªå…ˆå°ˆå±¬ï¼Œå¾Œå‚™é€šç”¨)
            category_name_match = re.search(r'Category: (.+?)\s*\(', header_line)
            category_name = category_name_match.group(1).strip() if category_name_match else ""
            normalized_name = clean_filename(category_name)

            solution_to_append = None
            solution_type = ""

            # 1. å„ªå…ˆæŸ¥æ‰¾å°ˆå±¬è§£æ³•
            if normalized_name in specific_solutions:
                solution_info = specific_solutions[normalized_name]
                solution_to_append = solution_info["content"]
                severity = solution_info["severity"]
                solution_type = f"å°ˆå±¬è§£æ³• (ä¾†æº: {severity.capitalize()})"

            # 2. è‹¥ç„¡å°ˆå±¬è§£æ³•ï¼ŒæŸ¥æ‰¾é€šç”¨è§£æ³•
            else:
                severity = get_severity_from_header(header_line)
                if severity and severity in generic_solutions:
                    solution_to_append = generic_solutions[severity]
                    solution_type = f"é€šç”¨è§£æ³• (ä¾†æº: {severity.capitalize()})"

            if solution_to_append:
                if append_solution_to_file(output_path, solution_to_append):
                    print(f"    -> å·²é™„åŠ  {solution_type}ã€‚")
                    solution_status = solution_type
                else:
                    print(f"    -> ({i}/{len(matches)}) å·²å¯«å…¥ '{filename}' (è§£æ³•é™„åŠ å¤±æ•—)")
                    solution_status = "è§£æ³•é™„åŠ å¤±æ•—"
            else:
                print(f"    -> ({i}/{len(matches)}) å·²å¯«å…¥ '{filename}' (ç„¡å°æ‡‰è§£æ³•)")
                solution_status = "ç„¡å°æ‡‰è§£æ³•"

            issue_summary_data.append({
                "index": i,
                "category_name": category_name,
                "solution_status": solution_status
            })
            files_processed_count += 1

        print(f"[SUCCESS] å°ˆæ¡ˆ '{project_name}' è™•ç†å®Œæˆï¼ç¸½å…±ç”¢å‡º {files_processed_count} å€‹å•é¡Œæª”æ¡ˆã€‚")

        # æ–°å¢ï¼šç”Ÿæˆä¸¦å„²å­˜å•é¡Œæ‘˜è¦å ±å‘Š
        summary_filename = f"{project_name}_issue_summary.md"
        summary_filepath = os.path.join(output_dir, summary_filename)

        summary_content = f"# {project_name} Fortify å•é¡Œæ‘˜è¦å ±å‘Š\n\n"
        summary_content += "## å•é¡Œåˆ—è¡¨èˆ‡è§£æ±ºæ–¹æ¡ˆç‹€æ…‹\n\n"
        summary_content += "| åºè™Ÿ | å•é¡Œé¡åˆ¥ | è§£æ±ºæ–¹æ¡ˆç‹€æ…‹ |\n"
        summary_content += "|---|---|---|\n"
        for item in issue_summary_data:
            summary_content += f"| {item['index']} | {item['category_name']} | {item['solution_status']} |\n"

        summary_content += "\n## èªªæ˜\n"
        summary_content += "- **è§£æ±ºæ–¹æ¡ˆç‹€æ…‹** é¡¯ç¤ºè©²å•é¡Œæ˜¯å¦åœ¨å…¬å…±ç­†è¨˜ä¸­æ‰¾åˆ°å°æ‡‰çš„è§£æ±ºæ–¹æ¡ˆã€‚\n"
        summary_content += "- **å°ˆå±¬è§£æ³•** è¡¨ç¤ºæ‰¾åˆ°é‡å°è©²ç‰¹å®šå•é¡Œçš„è§£æ±ºæ–¹æ¡ˆï¼Œä¸¦æ¨™ç¤ºå…¶ä¾†æºåš´é‡ç­‰ç´šã€‚\n"
        summary_content += "- **é€šç”¨è§£æ³•** è¡¨ç¤ºæ‰¾åˆ°é‡å°è©²å•é¡Œåš´é‡ç­‰ç´šçš„é€šç”¨è§£æ±ºæ–¹æ¡ˆã€‚\n"
        summary_content += "- **ç„¡å°æ‡‰è§£æ³•** è¡¨ç¤ºåœ¨å…¬å…±ç­†è¨˜ä¸­æœªæ‰¾åˆ°é‡å°è©²å•é¡Œçš„è§£æ±ºæ–¹æ¡ˆã€‚\n"

        with open(summary_filepath, 'w', encoding='utf-8') as f:
            f.write(summary_content)
        print(f"  [INFO] å·²ç”Ÿæˆå•é¡Œæ‘˜è¦å ±å‘Š: {summary_filepath}")

        return {"project_name": project_name, "issues": issue_summary_data}

    except Exception as e:
        print(f"\n  [ERROR] è™•ç† PDF '{pdf_filename}' éç¨‹ä¸­ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}")
        return None


def generate_global_summary(all_projects_data):
    """
    æ ¹æ“šæ‰€æœ‰å°ˆæ¡ˆçš„è³‡æ–™ï¼Œç”Ÿæˆä¸€ä»½å…¨åŸŸçš„å•é¡Œæ‘˜è¦å ±å‘Šã€‚
    """
    print("\n--- æ­£åœ¨ç”Ÿæˆ FIA å…¨åŸŸ Fortify å•é¡Œæ‘˜è¦å ±å‘Š ---")

    # ä½¿ç”¨ defaultdict ç°¡åŒ–çµ±è¨ˆé‚è¼¯
    # çµæ§‹: { 'category_name': {'count': 1, 'projects': {'proj1', 'proj2'}, 'solution': 'status'} }
    global_summary = defaultdict(lambda: {'count': 0, 'projects': set(), 'solution': 'ç„¡å°æ‡‰è§£æ³•'})

    # å®šç¾©è§£æ±ºæ–¹æ¡ˆç‹€æ…‹çš„å„ªå…ˆç´š
    solution_priority = {
        "è§£æ³•é™„åŠ å¤±æ•—": 0,
        "ç„¡å°æ‡‰è§£æ³•": 1,
        "é€šç”¨è§£æ³•": 2,
        "å°ˆå±¬è§£æ³•": 3
    }

    for project_data in all_projects_data:
        project_name = project_data['project_name']
        for issue in project_data.get('issues', []):
            category = issue['category_name']

            # æ›´æ–°çµ±è¨ˆæ•¸æ“š
            global_summary[category]['count'] += 1
            global_summary[category]['projects'].add(project_name)

            # æ›´æ–°è§£æ±ºæ–¹æ¡ˆç‹€æ…‹ (å–æœ€é«˜å„ªå…ˆç´šçš„)
            current_solution_status = global_summary[category]['solution']
            new_solution_status = issue['solution_status']

            # æå–ç‹€æ…‹é—œéµå­— (å°ˆå±¬/é€šç”¨/ç„¡)
            current_key = current_solution_status.split(' ')[0]
            new_key = new_solution_status.split(' ')[0]

            if solution_priority.get(new_key, 0) > solution_priority.get(current_key, 0):
                global_summary[category]['solution'] = new_solution_status

    if not global_summary:
        print("[INFO] æ²’æœ‰å¯ä¾›åˆ†æçš„è³‡æ–™ï¼Œä¸ç”Ÿæˆå…¨åŸŸå ±å‘Šã€‚")
        return

    # æº–å‚™ Markdown å ±å‘Šå…§å®¹
    report_path = os.path.join(PROJECT_ROOT, 'ç”¢å‡ºè³‡æ–™', 'Fortifyå ±å‘Šæ•´ç†', 'fia_fortify_issue_summary.md')

    content = "# FIA Fortify å•é¡Œç¸½çµå ±å‘Š\n\n"
    content += "æ­¤å ±å‘Šçµ±æ•´æ‰€æœ‰å·²åˆ†æå°ˆæ¡ˆçš„ Fortify å®‰å…¨å•é¡Œï¼Œæä¾›ä¸€å€‹é«˜å±¤æ¬¡çš„é¢¨éšªæ¦‚è¦½ã€‚\n\n"
    content += "## å•é¡Œé¡åˆ¥çµ±è¨ˆ\n\n"
    content += "| å•é¡Œé¡åˆ¥ (Category) | å‡ºç¾ç¸½æ¬¡æ•¸ | å—å½±éŸ¿å°ˆæ¡ˆ | è§£æ±ºæ–¹æ¡ˆç‹€æ…‹ |\n"
    content += "|---|---|---|---|\n"

    # æ ¹æ“šå‡ºç¾æ¬¡æ•¸æ’åº
    sorted_summary = sorted(global_summary.items(), key=lambda item: item[1]['count'], reverse=True)

    for category, data in sorted_summary:
        projects_str = ", ".join(sorted(list(data['projects'])))
        content += f"| {category} | {data['count']} | {projects_str} | {data['solution']} |\n"

    try:
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(content)
        print(f"[SUCCESS] å…¨åŸŸæ‘˜è¦å ±å‘Šå·²æˆåŠŸç”Ÿæˆæ–¼: {report_path}")
    except IOError as e:
        print(f"[ERROR] ç„¡æ³•å¯«å…¥å…¨åŸŸæ‘˜è¦å ±å‘Š: {e}")


def process_local_pdfs():
    """ä¸»åŸ·è¡Œå‡½å¼ï¼šæƒæPDFï¼Œæ‹†åˆ†å ±å‘Šï¼Œé™„åŠ è§£æ±ºæ–¹æ¡ˆï¼Œä¸¦ç”Ÿæˆå…¨åŸŸç¸½çµã€‚"""
    root_dir = REPORTS_DIR
    print("=== Fortify å ±å‘Šå…¨è‡ªå‹•è™•ç†è…³æœ¬å•Ÿå‹• ===")

    if not os.path.isdir(root_dir):
        print(f"[ERROR] æ‰¾ä¸åˆ°ä¾†æºç›®éŒ„ '{root_dir}'ï¼Œè«‹ç¢ºèªåŸ·è¡Œè·¯å¾‘æ˜¯å¦æ­£ç¢ºã€‚")
        return

    pdf_files = glob.glob(os.path.join(root_dir, '**', '*.pdf'), recursive=True)
    if not pdf_files:
        print(f"[INFO] åœ¨ '{root_dir}' ç›®éŒ„ä¸‹æ‰¾ä¸åˆ°ä»»ä½• PDF æª”æ¡ˆã€‚")
        return

    print(f"åœ¨ '{root_dir}' ä¸­æ‰¾åˆ° {len(pdf_files)} å€‹ PDFï¼Œæº–å‚™è™•ç†...")
    print("[INFO]    # æ­¥é©Ÿ 1: è¼‰å…¥é€šç”¨èˆ‡å°ˆå±¬è§£æ±ºæ–¹æ¡ˆ")
    generic_solutions, specific_solutions = load_solutions(os.path.join(PROJECT_ROOT, 'ç”¢å‡ºè³‡æ–™', 'Issueä¿®å¾©å…±ç­†'))
    if not generic_solutions and not specific_solutions:
        print("[INFO] æœªè¼‰å…¥ä»»ä½•è§£æ±ºæ–¹æ¡ˆï¼Œå°‡åƒ…åŸ·è¡Œå ±å‘Šæ‹†åˆ†ã€‚")
    else:
        print("[INFO] è§£æ±ºæ–¹æ¡ˆè¼‰å…¥å®Œæˆ.\n")

    all_projects_data = []
    for pdf_path in pdf_files:
        project_data = process_pdf_file(pdf_path, generic_solutions, specific_solutions)
        if project_data:
            all_projects_data.append(project_data)
        print("--- å°ˆæ¡ˆè™•ç†å®Œç•¢ ---\n")

    generate_global_summary(all_projects_data)
    print("=== æ‰€æœ‰ PDF å ±å‘Šè™•ç†å®Œæˆ ===")


if __name__ == "__main__":
    process_local_pdfs()